I"5Ñ<p>In a three-month summer internship at a small venture, I have learned basic NLP and implemented some interesting models.
Two major results include a shoddy Korean dialect translator made by a variation of HMM and an implementation of Seq2seq model with attention mechanism.</p>

<!-- more -->

<center><b>I. HMM on finite automata for dialect translation</b></center>

<p>Before beginning, I must notify that it did not show satisfiable performance.
In my defense, Korean dialect translation problem is extremely hard because of lack of data.
I have barely collected a small parallel corpus that consists of approximately 25,000 pair of Korean dialect-standard sentences.</p>

<p>The translator is basically based on the Hidden Markov model(HMM).
(Now writing‚Ä¶)</p>

<center><b>II. Implementation of Seq2seq with attention</b></center>

<p>The Seq2seq model is a neural network model for natural language translation. 
It uses the classical encoder-decoder structure with RNN cells and reflects the contexts of source language sentences by introducing an alignment mechanism, which is called ‚Äúattention‚Äù.
Alignment had been one of the main problems in statistical machine translation, and this model can be considered as the first application of alignment in neural network models.
The paper I implemented can be find <a href="https://arxiv.org/abs/1409.0473">here</a> $[1]$.</p>

<p>At first, import the pytorch package.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>The encoder part has been implemented as follows.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_vocab</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">Kx</span> <span class="o">=</span> <span class="n">n_vocab</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">embed_dim</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">H</span> <span class="o">=</span> <span class="n">hidden_dim</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">Kx</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">GRU</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">H</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span> <span class="c1"># [Tx,B]
</span>		<span class="n">embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="c1"># [Tx,B;E]&lt;-[Tx,B]
</span>		<span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gru</span><span class="p">(</span><span class="n">embeds</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span> <span class="c1">#: [Tx,B;2H],[2,B;H]&lt;-[Tx,B;E],[2,B;H]
</span>		<span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span> <span class="c1"># [Tx,B;2H],[2,B;H]
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>Here, $K_x$, $T_x$, $B$ denote the size of source language vocabulary, the maximum length of sentences, and the batch size respectively.
The class gets <code class="language-plaintext highlighter-rouge">input</code>, which has type of tensor of size $T_x\times B$, as an input.
This tensor is prepared to contain $B$ one-hot encoded sentences whose maximum length is $T_x$, and the empty spaces after the <code class="language-plaintext highlighter-rouge">eos</code> token are padded.
Then, it outputs output vectors obtained from each cell that will be used to make context vectors in alignment model and the last hidden vector that will be used to make initial hidden vector of decoder.
I have chosen GRU cell instead of vanila RNN cell or LSTM, and it is guessed that GRU is lighter than LSTM and does not lose its performance.
By calling <code class="language-plaintext highlighter-rouge">self.gru</code> only once, tensors propagate the circuit consisting of linearly connected <code class="language-plaintext highlighter-rouge">Tx</code> GRU cells.
The number <code class="language-plaintext highlighter-rouge">2</code> in <code class="language-plaintext highlighter-rouge">outputs</code> and <code class="language-plaintext highlighter-rouge">hidden</code> is due to the encoder cell is set to be bidirectional.</p>

<p>The main difficulty is the decoder.
The internal structure of a decoder cell can be drawn as follows:
<img src="http://localhost:4000/assets/svg/decoder.svg" style="width: 100%; height: : auto" />
It gets input tensor, hidden state tensor, and the tensor containing outputs from the encoder.
It returns output tensor and newly updated hidden state tensor.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_vocab</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">Ky</span> <span class="o">=</span> <span class="n">n_vocab</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">embed_dim</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">H</span> <span class="o">=</span> <span class="n">hidden_dim</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">Ky</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">GRU</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">H</span><span class="p">)</span> <span class="c1"># must be bidirectional=False
</span>		<span class="bp">self</span><span class="p">.</span><span class="n">lsm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
						<span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">Ky</span><span class="p">),</span>
						<span class="n">nn</span><span class="p">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
					<span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
						<span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">H</span><span class="p">),</span> <span class="c1"># [Tx,B;H]&lt;-[Tx,B;3H]
</span>						<span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">(),</span> <span class="c1"># [Tx,B;H]&lt;-[Tx,B;H]
</span>						<span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">H</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="c1"># [Tx,B;1]&lt;-[Tx,B;H]
</span>						<span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># [Tx,B;1]&lt;-[Tx,B;1]
</span>					<span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span> <span class="c1"># [B],[B;H],[Tx,B;2H]
</span>		<span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">alignment</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span> <span class="c1"># [B,2H]&lt;-[B;H],[Tx,B;2H]
</span>		
		<span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># [B;E]&lt;-[B]
</span>		<span class="n">rnn_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">embed</span><span class="p">,</span> <span class="n">context</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [B;E+2H]&lt;-[B;E],Context
</span>		<span class="n">rnn_input</span> <span class="o">=</span> <span class="n">rnn_input</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># [1,B;E+2H]&lt;-[B;E+2H]
</span>		<span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

		<span class="c1"># &gt;&gt; rnn_in[1,B;E+2H], hidden[1,B;H]
</span>		<span class="n">rnn_output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gru</span><span class="p">(</span><span class="n">rnn_input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
		<span class="n">rnn_output</span> <span class="o">=</span> <span class="n">rnn_output</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># [B;H]&lt;-[1,B;H]
</span>		<span class="c1"># &lt;&lt; rnn_out[B;H], hidden[1,B;H]
</span>
		<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">context</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [B;3H]&lt;-[B;H],Context
</span>		<span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lsm</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="c1"># [B;Ky]&lt;-[B;3H]
</span>		<span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># [B;Ky],[B;H]
</span>
	<span class="k">def</span> <span class="nf">alignment</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span> <span class="c1"># [B;H],[Tx,B;2H]
</span>		<span class="n">Tx</span> <span class="o">=</span> <span class="n">enc_output</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
		<span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">Tx</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [Tx,B;H]&lt;-[B;H]
</span>		<span class="n">attn_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span>
		<span class="n">attnw</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">attn</span><span class="p">(</span><span class="n">attn_input</span><span class="p">)</span> <span class="c1"># [Tx,B;1]&lt;-[Tx,B;3H]
</span>
		<span class="n">attnw</span> <span class="o">=</span> <span class="n">attnw</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># [B,1,Tx]&lt;-[Tx,B,1]
</span>		<span class="n">enc_output</span> <span class="o">=</span> <span class="n">enc_output</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [B,Tx,2H]&lt;-[Tx,B,2H]
</span>		<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attnw</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [B,2H]&lt;-[B,1,2H]&lt;-[B,1,Tx],[B,Tx,2H]
</span>		<span class="k">return</span> <span class="n">context</span> <span class="c1"># [B,2H]
</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>This GRU cell should not be bidirectional.
Unlike encoder, <code class="language-plaintext highlighter-rouge">self.gru</code> go through only one GRU cell.
It will be called <code class="language-plaintext highlighter-rouge">Ty</code> times when running a for loop of length <code class="language-plaintext highlighter-rouge">Ty</code> in <code class="language-plaintext highlighter-rouge">Seq2Seq.forward</code>.
The size of input <code class="language-plaintext highlighter-rouge">embed</code> vector is added by <code class="language-plaintext highlighter-rouge">2H</code> because context vector was attached.
The member <code class="language-plaintext highlighter-rouge">lsm</code> takes log-softmax for de-embedding and to get a log-probability distribution that the RNN output vector is corresponded to each taget language word.</p>

<p>Especially, <code class="language-plaintext highlighter-rouge">attn</code> has its unique role here.
Referring to hidden vector and the full-length output from encoder, it returns attention weights at each decoding time <code class="language-plaintext highlighter-rouge">ty</code>.
Note that the input length is <code class="language-plaintext highlighter-rouge">Tx</code>, not <code class="language-plaintext highlighter-rouge">Ty</code>.
The intput hidden vector is made by repeating the original decoder hidden of shape <code class="language-plaintext highlighter-rouge">[B;H]</code>, <code class="language-plaintext highlighter-rouge">Tx</code> times.
The encoder output is universally used at all decoding time <code class="language-plaintext highlighter-rouge">ty</code> wihtout modification.
Since the time <code class="language-plaintext highlighter-rouge">ty</code> is fixed in <code class="language-plaintext highlighter-rouge">decoder.forward</code>, the attention weights are not given by a matrix, but a <code class="language-plaintext highlighter-rouge">Tx</code>-dimensional real vector.</p>

<p>Lastly, the encoder and decoder cells are combined.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Seq2Seq</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_enc_vocab</span><span class="p">,</span> <span class="n">n_dec_vocab</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">Seq2Seq</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">n_enc_vocab</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">n_dec_vocab</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">init_hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
				<span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
				<span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">()</span>
			<span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">):</span> <span class="c1"># [Tx,B],Ty
</span>		<span class="n">Ty</span> <span class="o">=</span> <span class="n">trg_len</span>
		<span class="n">B</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
		<span class="n">Ky</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">Ky</span>
		<span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Ty</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">Ky</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

		<span class="c1"># encoding
</span>		<span class="n">enc_output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="c1"># [Tx,B;2H],[2,B;H]&lt;-[Tx,B]
</span>
		<span class="c1"># initialize inputs for decoder
</span>		<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="n">B</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># [B] SOS
</span>		<span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># [B;H]&lt;-[2,B;H]
</span>
		<span class="c1"># decoding
</span>		<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">Ty</span><span class="p">):</span>
			<span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
			<span class="n">outputs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">output</span>
			<span class="nb">input</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
		<span class="k">return</span> <span class="n">outputs</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>When instantiating the class, all attributes and their learnable paramters are automatically drawn in CUDA, if available.
The member <code class="language-plaintext highlighter-rouge">init_hidden</code> initializes hidden vector for decoding.
More precisely, it gets last hidden vector on reverse direction obtained from the encoder, and returns initial hidden vector for decoding.</p>

<p>For practical training, I used the <code class="language-plaintext highlighter-rouge">torchtext</code> package to implement the trainer class.
The type of <code class="language-plaintext highlighter-rouge">train_data</code> is list of tuple of tensors.
The size of each tuple is same with the number of fields, such as languages or lables.
In the following code, the data is prepared from a parallel corpus so that the data is a list of pairs of two tensors, and each tensor is a batch of one-hot encded sentences.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_data</span><span class="p">):</span>
	<span class="k">print</span><span class="p">(</span><span class="s">"[!] training model..."</span><span class="p">)</span>
	<span class="kn">import</span> <span class="nn">time</span>
	<span class="n">batch_loss</span> <span class="o">=</span> <span class="mi">0</span>
	<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
	<span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_data</span><span class="p">):</span>
		<span class="n">sources</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="n">src</span>
		<span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="n">trg</span>
		<span class="n">sources</span> <span class="o">=</span> <span class="n">sources</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># [Tx,B]
</span>		<span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># [Ty,B]
</span>		
		<span class="bp">self</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
		<span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">sources</span><span class="p">,</span> <span class="n">targets</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="c1"># forward propagation
</span>		<span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lossf</span><span class="p">(</span>
			<span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:].</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)),</span> <span class="c1"># [Ty*B;Kx]&lt;-[Ty,B;Kx]
</span>			<span class="n">targets</span><span class="p">[</span><span class="mi">1</span><span class="p">:].</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [Ty*B]&lt;-[Ty,B]
</span>		<span class="p">)</span>
		<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># back propagation
</span>		<span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">10.0</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
		
		<span class="n">batch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># loss is [1]-shape tensor
</span>		<span class="k">if</span> <span class="p">(</span><span class="n">b</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">40</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
			<span class="k">print</span><span class="p">(</span><span class="s">"[Batch : %4d/%4d] "</span><span class="o">%</span><span class="p">(</span><span class="n">b</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)),</span>
				  <span class="s">"[Avg Loss : %5.3f]"</span><span class="o">%</span><span class="p">(</span><span class="n">batch_loss</span><span class="o">/</span><span class="mi">40</span><span class="p">))</span>
			<span class="n">batch_loss</span> <span class="o">=</span> <span class="mi">0</span>
	<span class="k">print</span><span class="p">(</span><span class="s">"[TIME : %.2f"</span><span class="o">%</span><span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Although this model is for translation, with this model I could successfully made a simple sentiment analyzer that prints its polarity for each given Korean sentence.</p>

<center><b>References</b></center>

<p>$[1]$ D. Bahdanau, K. Cho, Y. Benglo, <i>Neural machine translation by jointly learning to align and translate,</i> arXiv preprint arXiv:1409.0473 (2014).</p>
:ET